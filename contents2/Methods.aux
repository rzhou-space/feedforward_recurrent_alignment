\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Methods}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Symmetric Recurrent Network Model}{1}{subsection.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces \textbf  {Illustration of symmetric recurrent networks (symmetric RNNs).}   \textbf  {(a)} An example of symmetric connections between two neurons in a network with multiple neurons. If there are connections between two neurons, here for example the green and red neurons, the directed connection from the green neuron to the red neuron has the same strength as the directed connection from the red to the green. \textbf  {(b)} Structure of a symmetric RNN with feedforward inputs as the inputs for the recurrent layer. The connections between neurons inside the recurrent layer are symmetric, as illustrated in Figure (a).\relax }}{1}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:symmetric_RNN}{{1.1}{1}{\textbf {Illustration of symmetric recurrent networks (symmetric RNNs).} \\ \textbf {(a)} An example of symmetric connections between two neurons in a network with multiple neurons. If there are connections between two neurons, here for example the green and red neurons, the directed connection from the green neuron to the red neuron has the same strength as the directed connection from the red to the green. \textbf {(b)} Structure of a symmetric RNN with feedforward inputs as the inputs for the recurrent layer. The connections between neurons inside the recurrent layer are symmetric, as illustrated in Figure (a).\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Symmetric Recurrent Interaction}{1}{subsubsection.4}\protected@file@percent }
\newlabel{sec:symmetric_recurrent_interaction}{{1.1.1}{1}{Symmetric Recurrent Interaction}{subsubsection.4}{}}
\newlabel{eq:gaussian_distribution}{{1.1}{1}{Symmetric Recurrent Interaction}{equation.5}{}}
\citation{dayan2005theoretical}
\newlabel{eq:eigval_normal}{{1.3}{2}{Symmetric Recurrent Interaction}{equation.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Response Steady State}{2}{subsubsection.8}\protected@file@percent }
\newlabel{sec:steady_state_response_sym}{{1.1.2}{2}{Response Steady State}{subsubsection.8}{}}
\@writefile{toc}{\contentsline {paragraph}{Existence of Steady State}{2}{section*.9}\protected@file@percent }
\newlabel{eq:sym_response_ODE}{{1.4}{2}{Existence of Steady State}{equation.10}{}}
\newlabel{eq:steady_state}{{1.5}{2}{Existence of Steady State}{equation.11}{}}
\@writefile{toc}{\contentsline {paragraph}{Stability of Steady State}{2}{section*.12}\protected@file@percent }
\citation{tragenap2023nature}
\newlabel{eq:Jacobian_matrix}{{1.8}{3}{Stability of Steady State}{equation.15}{}}
\newlabel{eq:steady_state_eigenvalues_sym}{{1.9}{3}{Stability of Steady State}{equation.17}{}}
\newlabel{eq:sym_stable_fix_point}{{1.10}{3}{Stability of Steady State}{equation.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.3}Feedforward Recurrent Alignment for Symmetric Interactions}{3}{subsubsection.19}\protected@file@percent }
\newlabel{sec:ffrec_definition}{{1.1.3}{3}{Feedforward Recurrent Alignment for Symmetric Interactions}{subsubsection.19}{}}
\newlabel{eq:ffrec_align}{{1.11}{3}{Feedforward Recurrent Alignment for Symmetric Interactions}{equation.20}{}}
\citation{tragenap2023nature}
\citation{Soch2019}
\citation{tragenap2023nature}
\newlabel{eq:h_prop_eigvec}{{1.12}{4}{Feedforward Recurrent Alignment for Symmetric Interactions}{equation.21}{}}
\newlabel{eq:ffrec_equals_eigval}{{1.13}{4}{Feedforward Recurrent Alignment for Symmetric Interactions}{equation.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.4}Response Properties for Evaluation}{4}{subsubsection.23}\protected@file@percent }
\newlabel{sec:response_properties_for_evaluation}{{1.1.4}{4}{Response Properties for Evaluation}{subsubsection.23}{}}
\newlabel{para:ttc_sym}{{1.1.4}{4}{Trial-to-trial correlation}{section*.24}{}}
\@writefile{toc}{\contentsline {paragraph}{Trial-to-trial correlation}{4}{section*.24}\protected@file@percent }
\newlabel{eq:input_distribution}{{1.14}{4}{Trial-to-trial correlation}{equation.25}{}}
\newlabel{eq:steady_state_distribute}{{1.15}{4}{Trial-to-trial correlation}{equation.26}{}}
\citation{tragenap2023nature}
\newlabel{eq:ttc_sym}{{1.18}{5}{Trial-to-trial correlation}{equation.29}{}}
\@writefile{toc}{\contentsline {paragraph}{Intra-trial stability}{5}{section*.30}\protected@file@percent }
\newlabel{eq:sde_intra_trial_stability}{{1.19}{5}{Intra-trial stability}{equation.31}{}}
\newlabel{eq:euler_maruyama}{{1.20}{5}{Intra-trial stability}{equation.34}{}}
\citation{tragenap2023nature}
\citation{tragenap2023nature}
\citation{tragenap2023nature}
\newlabel{eq:its_sym}{{1.24}{6}{Intra-trial stability}{equation.38}{}}
\@writefile{toc}{\contentsline {paragraph}{Dimensionality}{6}{section*.39}\protected@file@percent }
\newlabel{eq:input_distribution_dimensionality}{{1.25}{6}{Dimensionality}{equation.40}{}}
\newlabel{eq:response_distribution_dimensionality}{{1.26}{6}{Dimensionality}{equation.41}{}}
\newlabel{eq:Sigma_dim}{{1.27}{6}{Dimensionality}{equation.42}{}}
\newlabel{eq:descding_order}{{1.28}{6}{Dimensionality}{equation.43}{}}
\newlabel{eq:effective_dimensionality_analytical}{{1.29}{6}{Dimensionality}{equation.44}{}}
\newlabel{eq:eigval_sigma_dim}{{1.30}{7}{Dimensionality}{equation.45}{}}
\newlabel{eq:response_variance_ratio_dimensionality}{{1.31}{7}{Dimensionality}{equation.46}{}}
\newlabel{eq:dim_analytical_sym}{{1.32}{7}{Dimensionality}{equation.47}{}}
\newlabel{eq:dim_empirical_sym}{{1.33}{7}{Dimensionality}{equation.48}{}}
\@writefile{toc}{\contentsline {paragraph}{Alignment with spontaneous activity}{7}{section*.49}\protected@file@percent }
\citation{tragenap2023nature}
\newlabel{eq:var_explain_spont_act_sym}{{1.34}{8}{Alignment with spontaneous activity}{equation.50}{}}
\newlabel{eq:align_to_spont_act_sym}{{1.35}{8}{Alignment with spontaneous activity}{equation.51}{}}
\newlabel{eq:Sigma_spont}{{1.38}{8}{Alignment with spontaneous activity}{equation.54}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Asymmetric Recurrent Network Model}{9}{subsection.55}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces \textbf  {Illustration of asymmetric recurrent networks (asymmetric RNNs).} In general, asymmetric recurrent networks do not have symmetric interaction strength between two neurons. \textbf  {(a)} An example of asymmetric connections between two neurons in a network with multiple neurons. There are connections between the green and red neurons. The connection from green to red is stronger than the connection from red to green. Some connections are only from one neuron to the other but no connection back from the other neuron. \textbf  {(b)} Structure of an asymmetric RNN with feedforward inputs as the inputs for the recurrent layer. The connections between neurons inside the recurrent layer are asymmetric, as illustrated in Figure (a).\relax }}{9}{figure.caption.56}\protected@file@percent }
\newlabel{fig:asymmetric_RNN}{{1.2}{9}{\textbf {Illustration of asymmetric recurrent networks (asymmetric RNNs).} In general, asymmetric recurrent networks do not have symmetric interaction strength between two neurons. \textbf {(a)} An example of asymmetric connections between two neurons in a network with multiple neurons. There are connections between the green and red neurons. The connection from green to red is stronger than the connection from red to green. Some connections are only from one neuron to the other but no connection back from the other neuron. \textbf {(b)} Structure of an asymmetric RNN with feedforward inputs as the inputs for the recurrent layer. The connections between neurons inside the recurrent layer are asymmetric, as illustrated in Figure (a).\relax }{figure.caption.56}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Asymmetric Recurrent Interaction}{9}{subsubsection.57}\protected@file@percent }
\newlabel{sec:asym_recurrent_network}{{1.2.1}{9}{Asymmetric Recurrent Interaction}{subsubsection.57}{}}
\newlabel{eq:asym_interaction_matrix}{{1.39}{9}{Asymmetric Recurrent Interaction}{equation.58}{}}
\citation{rajan2006eigenvalue}
\newlabel{eq:asym_stable_fix_point}{{1.42}{10}{Asymmetric Recurrent Interaction}{equation.61}{}}
\newlabel{eq:asym_normalization}{{1.43}{10}{Asymmetric Recurrent Interaction}{equation.62}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces \textbf  {Eigenvalue distribution in dependence of parameter $a$ in eq.(\ref  {eq:asym_interaction_matrix}).} In general, an asymmetric matrix has eigenvalues in complex plane. The degree of symmetry determines the form of distribution from a line ($a = 1$, only real eigenvalues) to a full circle ($a = 0$) with radius $R < 1$. Between $a = 0$ and $a = 1$, the distribution is a symmetric ellipse along the line where imaginary part equals $0$. Subfigures from left to right show the eigenvalue distribution in the complex plane from $a = 1, 0.5,$ and $0$. The gray line marks the radius $-R$ and $R$. Here $R = 0.85 < 1$.\relax }}{11}{figure.caption.63}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}Modifications of Feedforward Recurrent Alignment for Asymmetric Interactions}{11}{subsubsection.64}\protected@file@percent }
\newlabel{sec:modify_ffrec_alignment_score}{{1.2.2}{11}{Modifications of Feedforward Recurrent Alignment for Asymmetric Interactions}{subsubsection.64}{}}
\newlabel{sec:modicication_real_part}{{1}{11}{Modifications of Feedforward Recurrent Alignment for Asymmetric Interactions}{Item.65}{}}
\newlabel{eq:ffrec_real_part}{{1.44}{11}{Modifications of Feedforward Recurrent Alignment for Asymmetric Interactions}{equation.66}{}}
\newlabel{sec:modification_magnitude}{{2}{11}{Modifications of Feedforward Recurrent Alignment for Asymmetric Interactions}{Item.67}{}}
\newlabel{eq:ffrec_mag}{{1.45}{11}{Modifications of Feedforward Recurrent Alignment for Asymmetric Interactions}{equation.68}{}}
\newlabel{sec:modification_symmetrized}{{3}{11}{Modifications of Feedforward Recurrent Alignment for Asymmetric Interactions}{Item.69}{}}
\newlabel{eq:symmetrized_J}{{1.46}{12}{Modifications of Feedforward Recurrent Alignment for Asymmetric Interactions}{equation.70}{}}
\newlabel{eq:ffrec_symmetrized}{{1.47}{12}{Modifications of Feedforward Recurrent Alignment for Asymmetric Interactions}{equation.71}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.3}Related Modifications for Evaluation}{12}{subsubsection.72}\protected@file@percent }
\newlabel{sec:modification_asym}{{1.2.3}{12}{Related Modifications for Evaluation}{subsubsection.72}{}}
\@writefile{toc}{\contentsline {paragraph}{Monotony}{12}{section*.73}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Trial-to-trial correlation}{12}{section*.74}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Intra-trial stability}{13}{section*.77}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Dimensionality}{13}{section*.81}\protected@file@percent }
\newlabel{eq:modifications_dim}{{1.51}{14}{Dimensionality}{equation.82}{}}
\newlabel{eq:modification_eff_dim}{{1.52}{14}{Dimensionality}{equation.83}{}}
\@writefile{toc}{\contentsline {paragraph}{Alignment to spontaneous activity}{14}{section*.84}\protected@file@percent }
\newlabel{eq:Sigma_spont_asym}{{1.53}{14}{Alignment to spontaneous activity}{equation.85}{}}
\citation{mastrogiuseppe2018linking}
\citation{mastrogiuseppe2018linking}
\citation{harris2013cortical}
\citation{mastrogiuseppe2018linking}
\citation{beiran2021shaping}
\citation{beiran2021shaping}
\citation{beiran2021shaping}
\citation{dubreuil2022role}
\citation{beiran2021shaping}
\citation{beiran2023parametric}
\citation{beiran2023parametric}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Low Rank Recurrent Network Model}{15}{subsection.86}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Construction of Low Rank Interactions}{15}{subsubsection.87}\protected@file@percent }
\newlabel{sec:low_rank_construct}{{1.3.1}{15}{Construction of Low Rank Interactions}{subsubsection.87}{}}
\@writefile{toc}{\contentsline {paragraph}{Low-rank RNNs without random noise \cite  {beiran2021shaping, dubreuil2022role}.}{15}{section*.88}\protected@file@percent }
\citation{beiran2021shaping}
\citation{mastrogiuseppe2018linking}
\citation{mastrogiuseppe2018linking}
\citation{mastrogiuseppe2018linking}
\citation{mastrogiuseppe2018linking}
\citation{mastrogiuseppe2018linking}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces \textbf  {Low-rank recurrent networks (RNNs) constructed with distinct Gaussian distribution \cite  {beiran2023parametric}}. A low-rank matrix could be written as a sum of outer products of vectors that are Gaussian distributed. As a result, the connectivity matrix is a mixture of Gaussians. \relax }}{16}{figure.caption.89}\protected@file@percent }
\newlabel{fig:low_rank_RNN_without_noise}{{1.4}{16}{\textbf {Low-rank recurrent networks (RNNs) constructed with distinct Gaussian distribution \cite {beiran2023parametric}}. A low-rank matrix could be written as a sum of outer products of vectors that are Gaussian distributed. As a result, the connectivity matrix is a mixture of Gaussians. \relax }{figure.caption.89}{}}
\newlabel{eq:low_rank_RNN_without_noise}{{1.54}{16}{Low-rank RNNs without random noise \cite {beiran2021shaping, dubreuil2022role}}{equation.90}{}}
\@writefile{toc}{\contentsline {paragraph}{Low-rank RNNs with random noise \cite  {mastrogiuseppe2018linking}.}{16}{section*.91}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces \textbf  {Low-rank recurrent networks (RNNs) constructed with fixed part and random noise \cite  {mastrogiuseppe2018linking}.} The connectivity matrix is given by the sum of a structured, controlled matrix $P$ and an uncontrolled, random matrix. Except for the fixed and known part $P$, which is a mixture of uncorrelated Gaussians, the RNN is disturbed by random noise.\relax }}{16}{figure.caption.92}\protected@file@percent }
\newlabel{fig:low_rank_RNN_with_noise}{{1.5}{16}{\textbf {Low-rank recurrent networks (RNNs) constructed with fixed part and random noise \cite {mastrogiuseppe2018linking}.} The connectivity matrix is given by the sum of a structured, controlled matrix $P$ and an uncontrolled, random matrix. Except for the fixed and known part $P$, which is a mixture of uncorrelated Gaussians, the RNN is disturbed by random noise.\relax }{figure.caption.92}{}}
\newlabel{eq:low_rank_with_noise}{{1.55}{17}{Low-rank RNNs with random noise \cite {mastrogiuseppe2018linking}}{equation.93}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}Feedforward Recurrent Alignment Hypothesis with Low-rank RNNs}{17}{subsubsection.94}\protected@file@percent }
\newlabel{sec:ffrec_low_rank}{{1.3.2}{17}{Feedforward Recurrent Alignment Hypothesis with Low-rank RNNs}{subsubsection.94}{}}
\@writefile{toc}{\contentsline {paragraph}{Symmetric low-rank RNN}{17}{section*.95}\protected@file@percent }
\newlabel{eq:sym_low_rank}{{1.56}{17}{Symmetric low-rank RNN}{equation.96}{}}
\@writefile{toc}{\contentsline {paragraph}{Asymmetric low-rank RNN}{17}{section*.97}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3}Evaluation of Feedforward Recurrent Alignment Hypothesis Based on Response Properties}{18}{subsubsection.98}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Symmetric low-rank RNNs}{18}{section*.99}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Asymmetric low-rank RNNs}{18}{section*.100}\protected@file@percent }
\citation{marre2009reliable}
\citation{mulholland2023selective}
\citation{marre2009reliable}
\citation{cosyne2023}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Black Box Recurrent Network Model}{19}{subsection.101}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}Approximation with White Noise Evoked Activity}{19}{subsubsection.102}\protected@file@percent }
\newlabel{sec:white_noise_approx_ffrec}{{1.4.1}{19}{Approximation with White Noise Evoked Activity}{subsubsection.102}{}}
\newlabel{eq:white_noise_evoked_act}{{1.58}{20}{Approximation with White Noise Evoked Activity}{equation.104}{}}
\newlabel{eq:ffrec_white_noise}{{1.59}{20}{Approximation with White Noise Evoked Activity}{equation.105}{}}
\citation{dayan2005theoretical}
\@writefile{toc}{\contentsline {paragraph}{Monotony}{21}{section*.106}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Trial-to-trial correlation}{21}{section*.107}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Intra-trial stability}{21}{section*.110}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Dimensionality}{22}{section*.114}\protected@file@percent }
\newlabel{eq:dim_white_noise}{{1.63}{22}{Dimensionality}{equation.115}{}}
\@writefile{toc}{\contentsline {paragraph}{Alignment between evoked activity and spontaneous activity}{22}{section*.119}\protected@file@percent }
\newlabel{eq:spont_act_white_noise}{{1.66}{22}{Alignment between evoked activity and spontaneous activity}{equation.121}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.2}Iterative Approximation with Low Dimensional Inputs}{22}{subsubsection.123}\protected@file@percent }
\newlabel{sec:repeat_low_dim_inputs}{{1.4.2}{22}{Iterative Approximation with Low Dimensional Inputs}{subsubsection.123}{}}
\citation{mulholland2023selective}
\newlabel{eq:low_dim_input_white_noise}{{1.69}{23}{Iterative Approximation with Low Dimensional Inputs}{equation.125}{}}
\newlabel{eq:repeat_low_dim_ffrec}{{1.71}{23}{Iterative Approximation with Low Dimensional Inputs}{equation.127}{}}
\newlabel{eq:update_ffrec}{{1.75}{24}{Iterative Approximation with Low Dimensional Inputs}{equation.131}{}}
\citation{dayan2005theoretical}
\citation{dayan2005theoretical}
\citation{dayan2005theoretical}
\citation{dayan2005theoretical}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Hebbian Learning in Feedforward Recurrent Networks}{25}{subsection.132}\protected@file@percent }
\newlabel{sec:Hebb_learn}{{1.5}{25}{Hebbian Learning in Feedforward Recurrent Networks}{subsection.132}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces \textbf  {Illustration of a general feedforward recurrent network construction.} Shown in the figure is a feedforward recurrent network with an input layer, an output layer, a feedforward synaptic weight matrix for feedforward interactions, and a recurrent synaptic weight matrix for recurrent interactions. Firing rate models are applied in both the input and output layer for modeling.\relax }}{25}{figure.caption.133}\protected@file@percent }
\newlabel{fig:feedforward_recurrent_network}{{1.6}{25}{\textbf {Illustration of a general feedforward recurrent network construction.} Shown in the figure is a feedforward recurrent network with an input layer, an output layer, a feedforward synaptic weight matrix for feedforward interactions, and a recurrent synaptic weight matrix for recurrent interactions. Firing rate models are applied in both the input and output layer for modeling.\relax }{figure.caption.133}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.1}Model Setting}{26}{subsubsection.134}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces \textbf  {Illustration of feedforward recurrent network model with single input neuron.} For start-up of feedforward recurrent network dynamic analysis, the simple case of only one input neuron with fixed random symmetric recurrent interaction $J$. The output layer has a number of $n$ neurons. The feedforward interaction matrix is updated with the Hebbian rule.\relax }}{26}{figure.caption.135}\protected@file@percent }
\newlabel{fig:single_neuron_feedforward_recurrent_network}{{1.7}{26}{\textbf {Illustration of feedforward recurrent network model with single input neuron.} For start-up of feedforward recurrent network dynamic analysis, the simple case of only one input neuron with fixed random symmetric recurrent interaction $J$. The output layer has a number of $n$ neurons. The feedforward interaction matrix is updated with the Hebbian rule.\relax }{figure.caption.135}{}}
\newlabel{eq:response_ffrec_network}{{1.76}{26}{Model Setting}{equation.136}{}}
\newlabel{eq:steady_eq_ffrec}{{1.77}{26}{Model Setting}{equation.137}{}}
\newlabel{eq:steady_state_ffrec}{{1.78}{27}{Model Setting}{equation.138}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.2}Update Rules for Feedforward Network}{27}{subsubsection.139}\protected@file@percent }
\newlabel{eq:basic_Hebb}{{1.79}{27}{Update Rules for Feedforward Network}{equation.140}{}}
\newlabel{eq:average_Hebb}{{1.80}{27}{Update Rules for Feedforward Network}{equation.141}{}}
\newlabel{eq:final_hebb_ff_weight}{{1.83}{27}{Update Rules for Feedforward Network}{equation.144}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.3}Projection of the Feedforward Weights on Eigenvectors}{28}{subsubsection.145}\protected@file@percent }
\newlabel{eq:euler_scheme_ffrec_weight}{{1.84}{28}{Projection of the Feedforward Weights on Eigenvectors}{equation.146}{}}
\newlabel{eq:linear_combi_weight}{{1.86}{28}{Projection of the Feedforward Weights on Eigenvectors}{equation.148}{}}
\newlabel{eq:calculation_ratio}{{1.87}{28}{Projection of the Feedforward Weights on Eigenvectors}{equation.149}{}}
\newlabel{eq:projection_ratio}{{1.88}{29}{Projection of the Feedforward Weights on Eigenvectors}{equation.151}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.4}Dynamics of Feedforward Recurrent Alignment}{29}{subsubsection.152}\protected@file@percent }
\newlabel{eq:feedforward_input}{{1.89}{29}{Dynamics of Feedforward Recurrent Alignment}{equation.153}{}}
\newlabel{eq:ffrec_update}{{1.90}{29}{Dynamics of Feedforward Recurrent Alignment}{equation.154}{}}
\newlabel{eq:derivative_ffrec}{{1.97}{31}{Dynamics of Feedforward Recurrent Alignment}{equation.163}{}}
\gdef \@abspage@last{31}
