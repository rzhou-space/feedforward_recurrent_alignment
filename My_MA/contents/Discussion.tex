\documentclass[11pt]{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{float}
\usepackage{lmodern,amsmath,amssymb,amstext,amsfonts,mathrsfs,graphicx,caption, subcaption}
\usepackage[width=14cm]{geometry}
\usepackage[colorlinks,pdfpagelabels,pdfstartview = FitV,bookmarksnumbered = true, bookmarksopenlevel=section, linkcolor = black,hypertexnames = false,citecolor = black,pdfpagelabels=false]{hyperref}
\usepackage{tablefootnote}
%\usepackage{rotating}
\usepackage{textcmds, enumitem}
\usepackage{sidecap} %, indentfirst
\usepackage[labelfont={bf,sf},font={small},labelsep=space]{caption}
\usepackage{chngcntr} % 			** Damit die Bilder Tabellen und Gleichungen 
\counterwithin{figure}{section}	%	** alle nach Kapiteln nummeriert sind.
\counterwithin{table}{section}%		**
\counterwithin{equation}{section}%	**	


\begin{document}
	\section{Discussion}
	% Summary/Conclusion -- want we find in the work. + Discussion 
	% Symmetry interaction
	\subsection*{Initial model for symmetric RNNs}
	The previous work from M. Kaschube lab and D. Fitzpatrick lab \cite{tragenap2023nature} that introduced the feedforward recurrent alignment hypothesis firstly. Their experimental results compared neural activities in visually naive and visually experienced ferrets' visual cortex, demonstrating that experience plays a critical role in increasing neural response consistency both across and within trials. Moreover, they found out that the  the initial evoked responses at eye opening consist of novel patterns, distinct from spontaneous activity of the endogenous network and that visual experience drives the development of low-dimensional, reliable representations aligned with spontaneous activity. 
	
	A computational model was developed with symmetric RNNs to give the proposal that high alignment of feedforward and recurrent networks can effectively amplify the novel activity patterns induced by the onset of visual experience \cite{tragenap2023nature}. The modeling results (section \ref{sec:results_symmetric}) support the experimental observations, indicating the potential of feedforward recurrent alignment hypothesis. Since symmetric RNNs are simple conceptual model for cortical networks, the investigation of broader network structures could improve the modeling and even give new insights to the understanding of underlying mechanism during experience-driven development. 
	
	% Asymmetry interaction
	\subsection*{Extension modifications for asymmetric RNNs}
	The closest extension of symmetric RNNs can be the asymmetric RNNs. Without the symmetry, the recurrent interactions can generally enable the alignment of feedforward and recurrent networks in complex plane. Complex patterns are however difficult to interpret in context of neuroscience. Therefore, modifications on the original model is necessary to overcome this problem. We consider three possible modifications and evaluat them based on the experimental and modeling results from the prior work from M. Kaschube lab and D. Fitzpatrick lab \cite{tragenap2023nature}. After the verification, the modification considering alignment of feedforward and symmetrized network has the best performance. 
	
	This indicates that in the case of general asymmetric RNNs, the alignment in the complex plane can be well approximated through symmetrization of the original RNNs. Besides, for full-rank asymmetric RNNs, symmetrization keeps the most of complex-plane information contained when transformed to real number among all three modifications. 
	
	% Low-rank interaction
	\subsection*{Test with low-rank RNNs}
	Beyond the full-rank RNNs, some recent work \cite{mastrogiuseppe2018linking, beiran2021shaping, dubreuil2022role} introduced low-rank interaction structures, which was predicted to capture the actual cortical connectivity more realistically than full-rank RNNs. Thus, we test the modeling on both symmetric and asymmetric RNNs with two possible constructions. The difference between two constructions is the dispersion from full-rank random noise. Symmetric low-rank RNNs coincide with the experimental observations with both constructions. If applying the best modification from full-rank asymmetric RNNs on low-rank asymmetric RNNs without random noise, the symmetrization lead to a duplication of alignment. 
	
	If considering the disturbance of random noise, the dynamics from low-rank RNNs will be overwritten and the results did not differ from full-rank RNNs. For asymmetric low-rank RNNs without random noise, other modification would be necessary to grantee functionality of the model on low-rank RNNs in general.  
	
	% white-noise evoked activity 
	\subsection*{Modification with white-noise evoked activity for black box RNNs}
	Considering the difficult accessibility of whole cortical interactions during experimental environment, we try to modify the modeling of feedforward recurrent alignment hypothesis with white-noise evoked activity as feedforward input to align with recurrent network. It turns out, that the experimental results can be roughly reflected. Besides, we test out the influence from repeatedly applying prior response activity as input on feedforward recurrent alignment. The dynamic of alignment indicate that the reuse of response as feedforward input to recurrent network can the response to a stable oscillation. 
	
	Therefore, white-noise evoked activity can be a good candidate for design of experimental methods to verify the feedforward recurrent alignment hypothesis. The underlying mechanism for stable oscillation when reusing prior response activity is still need to be analytically and experimentally explored. It can possibly give new insight for the feedforward recurrent alignment hypothesis and more understanding of feedforward recurrent network structure.
	
	% Hebbian learning
	\subsection*{Modeling under dynamic feedforward network with Hebbian learning}
	Since the development of ferrets' visual cortex was an experience-driven development, plasticity and leaning mechanisms cannot be ignored. For simplicity, starting modeling the feedforward recurrent alignment with dynamic feedforward network only. The classic Hebbian learning rule was considered. The feedforward recurrent alignment increas over time until steady state. 
	
	This indicates that during the learning, feedforward network adapts itself with unsupervised learning such that its outputs align better with recurrent network. According to the feedforward recurrent alignment hypothesis, this could imply that the responses become robust during learning. Therefore, the modeling with plasticity and learning rules can be a promising direction to explore the underlying mechanism for experimental observations of ferrets' visual cortex.
	
	\section{Outlook}
	% Outlook 1: Possible constructions of feedforward recurrent alignment that does not be influenced by complex eigenvectors.
	\subsection*{Possible constructions of feedforward recurrent alignment that does not be influenced by complex eigenvectors}
	In the case of general asymmetric RNNs, the problem that the alignment is in the complex plane emerge the modifications of the original model. The modifications considered in this work focus on turning the aligned feedforward input from complex plane into real-number plane, while possibly keeping the original feedforward recurrent alignment definition from \cite{tragenap2023nature}. 
	
	Another possible way for adapting the modeling of the feedforward recurrent alignment hypothesis could be a new construction of alignment score. The new construction could even allow the feedforward recurrent input align in complex plane but resulting the final score in real number. For example, in a form similar to
	\begin{equation} \label{eq:new_construction}
		\nu := (Jh)^*(Jh) = h^*J^*Jh = h^*JJh, 
	\end{equation} 
	where $h$ is the feedforward input and $J$ the recurrent interaction matrix. The operator $*$ denotes the conjugate transformed of vectors or matrices. The last equation is followed by the fact that interactions between neurons are represented by real numbers. 
	
	With the new construction eq.(\ref{eq:new_construction}), for both symmetric RNNs and asymmetric RNNs, the feedforward recurrent alignment is a real number when feedforward inputs are aligned to eigenvectors of interaction matrix. However, the monotony between eigenvalues and alignment score cannot be kept. %TODO: more explanation needed?
	Although this construction is not a candidate, it could provide some ideas for new construction of feedforward recurrent alignment to overcome the problems and bring new insights to feedforward recurrent alignment hypothesis in further studies. 
	
	% Outlook 2: Learning with normalization and also learning in recurrent network at the same time.
	\subsection*{Learning in recurrent network and learning rules with normalization}
	
	% Outlook 3: Considering excititory and inhibitory neuron populations for interaction structure.
	\subsection*{Interaction structure considering excititory and inhibitory neuron populations}
	
	
\end{document}