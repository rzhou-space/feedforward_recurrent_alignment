\documentclass[11pt]{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{float}
\usepackage{lmodern,amsmath,amssymb,amstext,amsfonts,mathrsfs,graphicx,caption, subcaption}
\usepackage[width=14cm]{geometry}
\usepackage[colorlinks,pdfpagelabels,pdfstartview = FitV,bookmarksnumbered = true, bookmarksopenlevel=section, linkcolor = black,hypertexnames = false,citecolor = black,pdfpagelabels=false]{hyperref}
\usepackage{tablefootnote}
%\usepackage{rotating}
\usepackage{textcmds, enumitem}
\usepackage{sidecap} %, indentfirst
\usepackage[labelfont={bf,sf},font={small},labelsep=space]{caption}
\usepackage{chngcntr} % 			** Damit die Bilder Tabellen und Gleichungen 
\counterwithin{figure}{section}	%	** alle nach Kapiteln nummeriert sind.
\counterwithin{table}{section}%		**
\counterwithin{equation}{section}%	**	
\begin{document}
	
	\section{Methods}
	% Introduction.
	In this chapter, we will give an overview about the recurrent network (RNN) models for exploration of the feedforward recurrent alignment hypothesis that are evolved in this work. The firstly introduced symmetric network model builds the basis for modifications and extensions in other further models. The modified models will be introduced subsequently. Finally, we consider the role of learning could play in the feedforward recurrent alignment hypothesis. 
	
	\subsection{Symmetric Recurrent Network Model}
	% General Information about symmetric interaction network. 
	Due to the well understood mathematical characters of symmetric RNNs, they are often applied in models for neuroscience for a better understanding of certain dynamics.  
	Therefore, we firstly consider the basic case of having a symmetric recurrent network, which has the symmetric interaction matrix. For symmetric RNNs, if there is a connection between two neurons $n_i$ and $n_j$, the strength of the directed connection from the neuron $n_i$ to the neuron $n_j$ equals the directed connection from $n_j$ to $n_i$. 
	% A figure for network construction.
		\begin{figure}[H] 
			%\centering
			\begin{subfigure}[b]{0.4\textwidth}
				\centering
				\includegraphics[width=0.2\textheight]{../figures/sym_net.pdf}
				\vspace{0.2cm}
				%\caption{Example of symmetric connections between two neurons.}
				\caption{}
			\end{subfigure}
			\hspace{0.4cm}
			\begin{subfigure}[b]{0.4\textwidth}
				\centering
				\includegraphics[width=0.4\textheight]{../figures/sym_recurrent.pdf}
				%\caption{Structure of a recurrent network (RNN)}
				\caption{}
			\end{subfigure}
		\caption{\textbf{Illustration of symmetric recurrent networks (symmetric RNNs).} \\ \textbf{(a)} An example of symmetric connections between two neurons in a network with multiple neurons. If there are connections between two neurons, here for example the green and red neurons, the directed connection from the green neuron to the red neuron has the same strength as the directed connection from the red to the green. \textbf{(b)} Structure of a symmetric RNN with feedforward inputs as the inputs for the recurrent layer. The connections between neurons inside the recurrent layer are symmetric, as illustrated in figure (a).}
		\label{fig:symmetric_RNN}
		\end{figure}
	
	\subsubsection{Symmetric Recurrent Interaction} \label{sec:symmetric_recurrent_interaction}
	% Definition of recurrent interaction. And how they constructed in the model.
	In the model, we consider a full rank real symmetric recurrent interaction matrix $J$ with Gaussian distributed entries with mean $0$ and variance $1$,
		\begin{equation} \label{eq:gaussian_distribution}
			{J_{ij}} \sim \mathcal{N}(0, 1).
		\end{equation}
	Besides, $J$ has full rank equals the number of neurons, 
		\begin{equation}
			\text{rank} (J) = n \, , 
		\end{equation}
	where $n$ is the number of neurons involved in the RNN. The eigenvalues $\{\lambda_i\}_{i = 1,...,n}$ of $J$ are limited by parameter $R < 1$ through
		\begin{equation} \label{eq:eigval_normal}
			\lambda_i = \frac{R \tilde{\lambda}_i}{\tilde{\lambda}_{\text{max}}} \, \, \, \, \, \forall i \, ,
		\end{equation}
	$\tilde{\lambda}_i$ are the original eigenvalues of $J$. As a result, the maximal eigenvalues after the re-scaling would take value $R <1$.  
	
	\subsubsection{Response Steady State} \label{sec:steady_state_response_sym}
	
	\paragraph{Existence of Steady State}
	% Inputs and steady state output setting. Output dynamics. Stability analysis.
	
%	The feedforward input vectors $h \in \mathbb{R}^{n \times 1}$ for the RNN consists of two parts: the deterministic part $h_{\text{det}} \in \mathbb{R}^{n \times 1}$ and the stochastic part $h_{\text{stoch}} \in \mathbb{R}^{n \times 1}$. So, 
%		\begin{equation}
%			h = h_{\text{det}} + h_{\text{stoch}} . 
%		\end{equation}
%	The stochastic part 
	When considering the relationship between firing rate and synaptic current as linear, the dynamic system of the RNN illustrated in \ref{fig:symmetric_RNN} could be described as \cite{dayan2005theoretical}:
		\begin{equation} \label{eq:sym_response_ODE}
			\tau_r \frac{\mathrm{d} r}{\mathrm{d} t} = -r + J \cdot r + h \overset{\tau_r = 1}{\Rightarrow} \frac{\mathrm{d} r}{\mathrm{d} t} = -r + J \cdot r + h\, , 
		\end{equation}
	with the vector $r \in \mathbb{R}^{n \times 1}$ describing responses of neurons in the recurrent layer, the vector $h \in \mathbb{R}^{n \times 1}$ as feedforward inputs ,and $\tau_r$ the time constant controlling the speed of dynamic. 
	The steady state of the dynamic system \ref{eq:sym_response_ODE} can be received by setting the ordinary differential equation to zero. For simplicity, the time constant is set to one. We then have
		\begin{equation} \label{eq:steady_state}
			\frac{\mathrm{d} r}{\mathrm{d} t} = -r + J \cdot r + h = 0 \Rightarrow r = (I_n - J)^{-1} \cdot h =: r^* \, ,
		\end{equation}
	$r^*$ the steady state for responses. $I_n \in \mathbb{R}^{n \times n}$ is the identity matrix. Since $J$ is full rank, the matrix $(I_n - J)$ is invertible. Therefore, the steady state exists.
	
	\paragraph{Stability of Steady State}
	The dynamic \ref{eq:sym_response_ODE} could also be written in an elementary expression:
		\begin{equation}
			 f_i (r_1, ..., r_n) := \frac{\mathrm{d} r_i}{\mathrm{d} t} = - r_i + \sum_{j=1}^{n} J_{ij} r_j + h_i  \text{   for } i = 1, ..., n\, .
		\end{equation}
	The derivative of $f_i$ to $r_j$ is
		\begin{equation}
			\frac{\partial f_i}{\partial r_j} = 
			\begin{cases}
				-1 + J_{ij} & \text{if} \, \,  i = j \\
				J_{ij} & \text{if} \, \, i \neq j
			\end{cases} \, \, .
		\end{equation}
	The Jacobian matrix $A$ of the dynamic system $\frac{\mathrm{d} r}{\mathrm{d} t}$ is then
		\begin{equation} \label{eq:Jacobian_matrix}
			A := 
			 \begin{pmatrix}
				\dfrac{\partial f_1}{\partial x_1} & \cdots & \dfrac{\partial f_1}{\partial x_n}\\
				\vdots                             & \ddots & \vdots\\
				\dfrac{\partial f_n}{\partial x_1} & \cdots & \dfrac{\partial f_n}{\partial x_n}
			\end{pmatrix}
			= - I_n + J \, .
		\end{equation}
	Therefore, the Jacobian matrix $A$ is a linear transformation of the symmetric recurrent interaction matrix $J$, which is independent of the steady state response. So, $A$ has the same set of eigenvectors \footnote{For a symmetric matrix, the set of left eigenvectors equal the set of right eigenvectors} as $J$. With $E := \{e_i\}_{i = 1, ..., n}$ the matrix containing eigenvectors of $J$ column-wise, 
		\begin{equation} \label{eq:steady_state_eigenvalues_sym}
			(- I_n + J) E = - I_n \cdot E + J \cdot E = -I_n \cdot E + \Lambda \cdot E = (-I_n + \Lambda) E \, ,
		\end{equation}
	$\Lambda$ the diagonal matrix with eigenvalues $\{\lambda_i\}_{i = 1,...,n}$ of $J$ on its diagonal. This means, $\{-1 + \lambda_i\}_{i=1, ..., n}$ are eigenvalues for the Jacobian matrix A. 
	
	The eigenvalues of the Jacobian matrix $A$ determines the stability of steady states. Here, since the matrix $A$ is symmetric, all its eigenvalues $-1 + \lambda_i, i = 1, ..., n$ are from $\mathbb{R}$. Because the eigenvalues $\lambda_i$ of matrix $J$ is limited by the parameter $R<1$, defined in \ref{eq:eigval_normal}, we have 
		\begin{equation} \label{eq:sym_stable_fix_point}
			-1 + \lambda_i \overset{(\ref{eq:eigval_normal})}{<} -1 + 1 = 0 \, .
		\end{equation}
	That is, all eigenvalues of the Jacobian matrix $A$ are negative. This indicates that the steady state $r^*$ is stable. Under the assumption that the system reaches its steady state quick enough, we could apply the steady state $r^*$ for further analysis. 
	
	\subsubsection{Feedforward Recurrent Alignment for Symmetric Interactions} \label{sec:ffrec_definition}
	% TODO: The meaning of the feedforward recurrent alignment hypothesis (perhaps in the introduction better) 
	%In which case will it be applied.
	%TODO: The clarifying of mean firing rate in introduction perhaps?
	Generally, the feedforward inputs can be considered as a firing rate distribution with certain mean value. The mean firing rate is essential for the strength of inputs. We therefore consider mainly the mean firing rate of inputs. For the rest of work, if mentioning feedforward inputs without further definition, we mean the mean firing rate of inputs. 
	
	The alignment of a feedforward input $h \in \mathbb{R}^{n \times 1}$ with the recurrent network $J$ is defined as \cite{tragenap2023nature}
		\begin{equation} \label{eq:ffrec_align}
			\nu := \frac{h^T J h}{\parallel h \parallel_2 ^2}
		\end{equation}	
	If the inputs are aligned to the eigenvectors $e_i$ of the recurrent interaction $J$, i.e., 
		\begin{equation} \label{eq:h_prop_eigvec}
			h \propto e_i  \, ,
		\end{equation}
	the feedforward recurrent alignment $\nu$ is proportional to the eigenvalues $\lambda_i$, because inserting the proportionality (\ref{eq:h_prop_eigvec}) in (\ref{eq:ffrec_align}) leads to
		\begin{equation} \label{eq:ffrec_equals_eigval}
			\nu = \frac{h^T J h}{\parallel h \parallel_2 ^2} \propto \frac{{e_i}^T J e_i}{\parallel e_i \parallel_2 ^2} = \frac{\lambda_i {e_i}^T e_i}{\parallel e_i \parallel_2 ^2} = \lambda_i \, .
		\end{equation}
	It was therefore observed that the maximal alignment was attained when the input was proportional to the eigenvector $e_{\text{max}}$ with maximal eigenvalue $\lambda_{\text{max}}$ \cite{tragenap2023nature}. 
	
	\subsubsection{Response Properties for Evaluation} \label{sec:response_properties_for_evaluation}
	% TODO:general introduction where does this come? Or should it be in the introduction instead?
	% trial to trial correlation, intra-trial stability, dimensionality and alignment to spontaneous activity.
	\paragraph{Trial-to-trial correlation} \label{para:ttc_sym}
	Given the feedforward inputs that are from the same distribution for multiple trials, the correlation between responses from different trials indicates the reliability of the responses. Large correlation implies high reliability of the response generated by the RNN. 
	
	Model the inputs $h \in \mathbb{R} ^{n \times 1}$ as multivariate normal distributions with mean vector $\mu \in \mathbb{R}^{n \times 1}$ and covariance matrix $\Sigma \in \mathbb{R}^{n \times n}$
		\begin{equation} \label{eq:input_distribution}
			h \sim \mathcal{N} (\mu, \Sigma) \, \text{ with } \Sigma := \sigma_{\text{trial}}I_n. 
		\end{equation}
	Then, the steady state response $r^* = (I_n - J)^{-1} \cdot h $ from (\ref{eq:steady_state}) has the linearly transformed normal distribution
		\begin{equation} \label{eq:steady_state_distribute}
			r^* \sim \mathcal{N} \left((I_n - J)^{-1}\mu, (I_n - J)^{-1} \Sigma (I_n -J)^{-T}\right) \, ,
		\end{equation}
	where the mean vector and covariance matrix are linearly transformed. The property could be proved analogously as in \cite{Soch2019} with the moment-generating function of the multivariate normal distribution
		\begin{equation}
			M_h(t) = \mathbb{E}\left[\text{exp}(t^Th)\right] = \text{exp} \left[t^T \mu + \frac{1}{2} t^T \Sigma t \right] \, .
		\end{equation}
	Therefore, the moment-generating function of the random vector $r^*$ becomes
		\begin{equation}
			\begin{split}
				M_{r^*} & = M_h \left( (I_n-J)^{-T} t\right) \\
				        & = \text{exp} \left[
				         						t^T \left( (I_n - J)^{-1} \mu\right) + \frac{1}{2} t^T (I_n - J)^{-1} \Sigma (I_n - J)^{-T} t    
				         				\right]
			\end{split} \, ,
		\end{equation}
	which indicates the linearly transformed distribution of $r^*$ as in (\ref{eq:steady_state_distribute}).
	
	As calculated in \cite{tragenap2023nature}, the trial to trial correlation $\beta$ for one stimulus $s$ is calculated by taking the mean of correlations between $N$ response trials that evoked by this stimulus. That is
		\begin{equation} \label{eq:ttc_sym}
			\beta_s = \frac{2}{N(N-1)} \sum_{i = 1, j = i+1}^{N} corr(r_i^s, r_j^s) \, ,
		\end{equation}
	where $r_i^s$ is the $i$-th response trial that evoked by stimulus $s$. 
	
	\paragraph{Intra-trial stability}
	
	It was observed that presenting ongoing visual grating stimuli, the responses in the visually naive cortex has a stronger variation than they are after visual experience. In order to reflect the variation of responses during the stimulation period, the quantity of "intra-trial stability" was defined \cite{tragenap2023nature}. 
	
	To model the time dependent input $h(t) \in \mathbb{R}^{n \times 1}$ distributed as (\ref{eq:input_distribution}) and its evoked steady state responses $r(t) \in \mathbb{R}^{n \times 1}$, the following stochastic differential equations were formulated
		\begin{subequations} \label{eq:sde_intra_trial_stability}
			\begin{align}
				\mathrm{d} h & = \mu \mathrm{dt} + \sigma_{\text{time}} \mathrm{d} W \\
				\mathrm{d} r & = (-r + J \cdot \mu) \mathrm{d}t + \sigma_{\text{time}} \mathrm{d} W \, ,
			\end{align}
		\end{subequations}
	with $W$ the Wiener process, which is a continuous-time stochastic process with independent Gaussian increments. 
	
	To approximate the evoked response $r(t)$, the equation (\ref{eq:sde_intra_trial_stability}b) is solved numerically with Euler-Maruyama scheme 
		\begin{equation}
			r_{t+1} = r_t + (-r_t + J \cdot \mu) \Delta t + \sigma_{\text{time}} \sqrt{\Delta t} \Delta \tilde{W}_t \, ,
		\end{equation}
	with $r_t$ the response at time point $t$, $\Delta t$ the step width for iteration, and $\Delta \tilde{W}_t \in \mathbb{R}^{n \times 1}$ the Gaussian increment at time point $t$ defined as the multivariate normal distribution with mean vector $0_v$ and covariance matrix $I_n$
		\begin{equation}
			\Delta \tilde{W}_t \sim \mathcal{N}(0_v, I_n) \, .
		\end{equation}
	
	For a certain step width $\tilde{\Delta t}$, the intra-trial stability $c(\Delta \tilde{t})$ was defined by the correlation between normalized response at time $t$ and its delayed response at time $t + \Delta \tilde{t}$ 
		\begin{equation}
			c(\Delta \tilde{t}) := \bar{r}(t)^T \bar{r}(t + \Delta t) \, , 
		\end{equation}
	where the normalized response is defined as
		\begin{equation}
			\bar{r}(t) := \frac{r - \langle r \rangle}{\sigma_r} \, ,
		\end{equation}
	with mean value of $r$ denoted by $\langle r \rangle$ and standard deviation by $\sigma_r$. 
	
	The final intra-trial stability for a time period $T$ is the time-averaged value over all time points $0 \leq t \leq T - \Delta \tilde{t}$
		\begin{equation} \label{eq:its_sym}
			\begin{split}
				\bar{c}{(\Delta \tilde{t})} &= \frac{1}{T-\Delta \tilde{t}} \int_{0}^{T-\Delta \tilde{t}} c(\Delta \tilde{t}) \mathrm{d} t\\
				                            &= \frac{1}{T-\Delta \tilde{t}} \int_{0}^{T-\Delta \tilde{t}} \bar{r}(t)^T \bar{r}(t + \Delta t) \mathrm{d} t \, .
			\end{split} 
		\end{equation}
	
	\paragraph{Dimensionality}
	%TODO: Sources for dimensionality meaning and linear dimensionality definition.
	
	The dimensionality of neuron responses reflect the complexity of the informatbetaion they encoded. %TODO: literature.
	A diverse response pattern corresponds with a broader distribution of the variance over principal components, leading to a higher-dimensional linear manifold. %TODO: lietrature
	The corresponding dimensionality for more diverse and variable response pattern is therefore higher \cite{tragenap2023nature}. %TODO: Is the understanding correct? Further literatures?
	Given the multivariate normal distributed inputs $h \in \mathbb{R}^{n \times 1}$
		\begin{equation} \label{eq:input_distribution_dimensionality}
			h \sim \mathcal{N}(0_v, \Sigma^{\text{Dim}}) \, ,
		\end{equation}
	the linear transformed responses (analogously as (\ref{eq:steady_state_distribute})) are
		\begin{equation} \label{eq:response_distribution_dimensionality}
			r \sim \mathcal{N}(0_v, (I_n - J)^{-1} \Sigma^{\text{Dim}} (I_n - J)^{-T})
		\end{equation}
	with 
		\begin{equation} \label{eq:Sigma_dim}
			\Sigma^{\text{Dim}} := \sum_{i=L}^{L+M} \text{exp}\left(\frac{2(i-L)}{\beta}\right) e_i e_i^T \, , 
		\end{equation}
	in which the parameter $M := \kappa \beta$ and $\beta$ reflects the dimensionality \cite{tragenap2023nature} and $\kappa$ for determining the number of directions $e_i$ that contribute to the dimensionality. Since the eigenvectors of $J$ build a set of basis for $\mathbb{R}^n$, they could be chosen as $e_i$ for (\ref{eq:Sigma_dim}). Hereby, the eigenvectors are ordered according to their eigenvalues in descending order. The exponential factor in (\ref{eq:Sigma_dim}) simulates the exponential decay of variance ratio observed in prior data \cite{tragenap2023nature}. 
	
	The linear effective dimensionality based on participation ratio was defined %TODO: literature
	to quantify the tendency of dimensionality during visual maturation.
	The participation ratio is defined as %TODO: literature.
		\begin{equation} \label{eq:effective_dimensionality_analytical}
			d_{\text{eff}} := \frac{\left(\sum_{i} \lambda_i\right)^2}{\left( \sum_{i} \lambda_i^2\right)} \, ,
		\end{equation}
	where $\lambda_i$ the eigenvalues of a certain response pattern with covariance $\mathbf{\Sigma}$. Since as defined in (\ref{eq:Sigma_dim}), $\Sigma^{\text{Dim}}$ has the same eigenvectors (aka. principal components )as $J$. Therefore, the eigenvalues $\lambda_i^{\text{Dim}}$(aka. variance ratio or variance explained) for $\Sigma^{\text{Dim}}$ are transformed eigenvalues $\lambda_i$ of $J$ expressed as
		\begin{equation}
			\lambda_i^{\text{Dim}} = \text{exp}\left(\frac{2(i-L)}{\beta}\right) \lambda_i \, .
		\end{equation}
	The covariance of the responses share the same eigenvectors as $\Sigma^{\text{Dim}}$ based on its distribution (\ref{eq:response_distribution_dimensionality}) and therefore also the same as $J$. The eigenvalues (aka. variance ratio) for the responses could be constructed analogously with
		\begin{equation} \label{eq:response_variance_ratio_dimensionality}
			\lambda_i^{\text{Act}} = \text{exp}\left(\frac{2(i-L)}{\beta}\right) \frac{1}{(1-\lambda_i)^2} \, ,
		\end{equation}
	for $i = L, ..., L+M$, due to the inverse transformation. 
%	 For the covariance $\Sigma^{\text{Dim}}$ from inputs, if having its eigenvalues $\lambda_i^{\text{Dim}}$, the eigenvalues $\lambda_i^{\text{Act}}$ of the steady state responses could be transformed based on the distribution (\ref{eq:response_distribution_dimensionality}). It leads to the following relation
%		\begin{equation}
%			\lambda_i^{\text{Act}} = \frac{1}{(1-\lambda_i^{\text{Dim}})^2} = \, . %TODO: the exp factors
%		\end{equation}
%	Therefore, the 
	
	Insert the eigenvalues of responses (\ref{eq:response_variance_ratio_dimensionality}) in the formula for effective dimensionality to get the final formulation of dimensionality for responses
		\begin{equation} \label{eq:dim_analytical_sym}
			d^r_{\text{eff}} = \frac{\left(\sum_{i = L}^{L + M} \text{exp}\left(-2 \frac{i-L}{\beta}\right)(1-\lambda_i)^{-2}\right)^2}{\sum_{i=L}^{L+M} \text{exp}\left(-4 \frac{i-L}{\beta}\right)(1-\lambda_i)^{-4}}
		\end{equation}
	
	Since the vector of explained variance ratios in the principal component analysis (PCA) is the normalized vector containing eigenvales of the covariance matrix re-scaled by the largest eigenvalue in descending order, which then explains how much variance does the corresponding principal component contribute. Therefore, another way to access the dimensionality is to empirically determine the explained ratio of generated data samples through PCA and insert the variance ratio into the definition of effective dimentionality, i.e.,
		\begin{equation} \label{eq:dim_empirical_sym}
			d_{\text{eff}} = \frac{\left(\sum_{i=L}^{L+M}\text{var}_i\right)^2}{\sum_{i=L}^{L+M}\text{var}_i^2} 
		\end{equation}
	with $\text{var}_i$ the $i$-th variance ratio. 
	
	\paragraph{Alignment with spontaneous activity}
	
	The alignment of activity patterns to spontaneous activity reflects in principle the size of overlaps between activity patterns and spontaneous activity pattern. 
	% is in principle the dimensionality explained by the principal components of spontaneous activity. 
	Assuming having the evoked response pattern as $R$ and the spontaneous activity pattern as $S$, the projection of $R$ to $S$ could be quantified as the covariance matrix of $R$ explained by the principal components (a.k.a. eigenvectors of covariance matrix) of $S$, which results a variance ratio vector
	%we then have the variance of activity pattern $R$ explained by principal components of pattern $S$ expressed as a vector $\mathbf{v}$ with 
		\begin{equation} \label{eq:var_explain_spont_act_sym}
			\mathbf{v}_i = \frac{\mathbf{p}_{i, S}^T \cdot \mathbf{\Sigma_R} \cdot \mathbf{p}_{i, S}}{\text{Tr}(\mathbf{\Sigma_R})} \, , 
		\end{equation}
	for $i = 1, ..., n$. $\mathbf{p}_{i, S}$ are the principal component of spontaneous activity and $\mathbf{\Sigma_R}$ the covariance matrix of evoked activity. 
	%TODO: the connections between variance explained and alignment. 
	%It was observed in \cite{tragenap2023nature} that patterned visual experience is required to cause stable alignment between visual responses and the spontaneous activities, while naive visual responses show only loose alignment with spontaneous activities. To capture this change of property as a quantity instead of a vector as (\ref{label}), it had to firstly define the alignment between two activity patterns. The alignment of $R$ to $S$ was defined as the average of pattern $S$ explained by normalized trials $r_{i, R}$. The alignment between pattern $S$ and one trial $r_{i, R}$ from $R$ is defined as
	Consider the projection of activity pattern $R$ in all directions of spontaneous pattern $S$ together to reflect the overall overlaps between two patterns, we calculate firstly the alignment between a response trial $r_{i, R}$ form $R$ to the spontaneous pattern $S$ 
		\begin{equation} \label{eq:align_to_spont_act_sym}
			\gamma_i = \frac{r^T_{i, R} \cdot \Sigma_S \cdot r_{i,R}}{\Vert r^T_{i, R} \Vert^2 \text{Tr}(\Sigma_S)}\, ,
		\end{equation}
	where $\Sigma_S$ is the covariance of pattern $S$. The final alignment between $S$ and $R$ is then the average value of alignment between $S$ and all trials of $R$. 
	
	To model the inputs and responses, we assumed that spontaneous activity was evoked by inputs from broad sources. Besides, since the spontaneous activity already exists almost a week before eye opening, we assume that they already fit to the activity space generated by recurrent network \cite{tragenap2023nature}. Therefore, the inputs would be explained by more directions (or eigenvectors) than stimuli evoked responses as modeled before with (\ref{eq:input_distribution_dimensionality}), that is higher dimensionality. Since the parameter $\beta$ in (\ref{eq:input_distribution_dimensionality}) indicates the dimensionality, we could set for spontaneous activity higher $\beta_{\text{spont}} > \beta$ to generate high dimensional inputs. 
	Therefor, we than have the broad inputs $h^{\text{spont}} \in \mathbb{R}^{n \times 1}$ and spontaneous activity $r^{\text{spont}} \in \mathbb{R}^{n \time 1}$, which are multivariate distributed vectors 
		\begin{equation}
			\mathbf{h}^{\text{spont}} \sim \mathbf{\mathcal{N}}(\mathbf{0_v}, \mathbf{\Sigma}^{\text{spont}})
		\end{equation}
	and 
		\begin{equation}
			\mathbf{r}^{\text{spont}} \sim \mathbf{\mathcal{N}}\left( \mathbf{0_v}, (\mathbf{I_n} - \mathbf{J})^{-1} \mathbf{\Sigma}^{\text{spont}}(\mathbf{I_n} - \mathbf{J})^{-T}\right) \, .
		\end{equation}
	The covariance matrix $\mathbf{\Sigma}^{\text{spont}}$ is constructed in the same way as $\mathbf{\Sigma}^{\text{Dim}}$ only with $L=1$ and larger $\beta_{\text{spont}}$, that is
		\begin{equation} \label{eq:Sigma_spont}
			\mathbf{\Sigma}^{\text{spont}} := \sum_{i=1}^{\kappa\beta_{\text{spont}}} \text{exp}\left(\frac{2(i-1)}{\beta_{\text{spont}}}\right) e_i e_i^T \, .
		\end{equation}
	
	\clearpage
	\subsection{Asymmetric Recurrent Network Model}
	% Reason for considering asymmetric networks.
	Different as symmetric recurrent networks, asymmetric recurrent networks do not necessary have symmetric interaction strength between two neurons. That is, if we have two neurons $n_i$ and $n_j$, the interaction strength from $n_i$ to $n_j$ can generally differ from the interaction strength from $n_j$ to $n_i$. Therefore, the network has a more complex neural interactions and dynamics. Compare to symmetric recurrent network, the asymmetric recurrent network reflects more general neural interactions and thus more biologically realistic. 
	
	\begin{figure}[H] 
		%\centering
		\begin{subfigure}[b]{0.4\textwidth}
			\centering
			\includegraphics[width=0.2\textheight]{../figures/asym_net.pdf}
			\vspace{0.2cm}
			%\caption{Example of symmetric connections between two neurons.}
			\caption{}
		\end{subfigure}
		\hspace{0.4cm}
		\begin{subfigure}[b]{0.4\textwidth}
			\centering
			\includegraphics[width=0.4\textheight]{../figures/asym_recurrent.pdf}
			%\caption{Structure of a recurrent network (RNN)}
			\caption{}
		\end{subfigure}
		\caption{\textbf{Illustration of asymmetric recurrent networks (asymmetric RNNs).} In general, asymmetric recurrent networks do not have symmetric interaction strength between to neurons. \textbf{(a)} An example of asymmetric connections between two neurons in a network with multiple neurons. There are connections between the green and red neurons. The connection from green one to red one is stronger than the connection from the red to green. There are also connections that only from one neuron to the other but no connection back from the other neuron.  \textbf{(b)} Structure of a asymmetric RNN with feedforward inputs as the inputs for the recurrent layer. The connections between neurons inside the recurrent layer are asymmetric, as illustrated in figure (a).}
		\label{fig:asymmetric_RNN}
	\end{figure}
	
	
	\subsubsection{Asymmetric Recurrent Interaction}
	% Definition of the asymmetric recurrent interaction network. The problems that come up with the asymmetricity.
	For the modeling, we construct the asymmetric interaction matrix $J$ through disturbing symmetric interaction matrix by a general asymmetric random matrix
		\begin{equation} \label{eq:asym_interaction_matrix}
			J = a J_{\text{sym}} + (1-a) J_{\text{asym}} \, .
		\end{equation}
	The symmetric part is generated as described in section \ref{sec:symmetric_recurrent_interaction}. The asymmetric part has also Gaussian distributed entries with mean $0$ and $1$ as variance (\ref{eq:gaussian_distribution}). Parameter $a$ therefore indicates the degree of symmetry in the network. 
	
	The recurrent network dynamics remains the same as in symmetric case, described by (\ref{eq:sym_response_ODE}). Therefore, the steady state also keeps its form as (\ref{eq:steady_state}). Stability analysis of the steady state follows the same procedure as for symmetric interaction matrix (section \ref{sec:steady_state_response_sym}). However, here we have to mind that the asymmetric interaction matrix $J$ now have left and right eigenvectors, which are not identical to each other any more. The Jacobian matrix $A = -I_n + J$ defined from (\ref{eq:Jacobian_matrix}) still has the same set of eigenvectors as $J$. The eigenvalues are found in the similar way as (\ref{eq:steady_state_eigenvalues_sym}) but with left and right eigenvectors. If $E_l$ and $E_r$ the matrices containing the left and right eigenvectors column-wise, it follows
		\begin{equation}
			\begin{split}
				E_l^*(-I_n + J) &= - E_l^* + E_l^* J =  - E_l^* + \Lambda E_l^* = (-I_n + \Lambda) E_l^* \\
				(-I_n + J) E_r &= - E_r + J E_r = -E_r + E_r \Lambda = E_r (-I_n + \Lambda) \, , 
			\end{split}
		\end{equation}	
	with $E_l^*$ the conjugate transport matrix of $E_l$, $I_n$ the identity matrix, and $\Lambda$ the diagonal matrix that contains eigenvalues of $J$ on its diagonal. The eigenvalues $\{\lambda_i\}$ for $J$ are in general complex numbers
	\begin{equation}
		\{\lambda_i \in \mathbb{C} \mid \lambda_i \text{ eigenvalues of $J$}\} \, .
	\end{equation}
	Therefore, the eigenvalues for the Jacobian matrix $A$ is now $-I_n + \Lambda \in \mathbb{C}^{n \times n}$. 
	
	To determine the stability of steady state, we now have to consider the real part of $(-I_n + \Lambda)$. If $\exists i: -1 + \text{Re}(\lambda_i) > 0$, the steady state will be unstable. However, if $\forall i: -1 + \text{Re}(\lambda_i) < 0$, the steady state is then stable and we will have
		\begin{equation} \label{eq:asym_stable_fix_point}
			\forall i: -1 + \text{Re}(\lambda_i) < 0 \, \,  \Rightarrow \, \, \text{lim}_{t \rightarrow \infty} r(t) = r^*
		\end{equation}
	Thus, we need to limit the range of eigenvalues for $J$ such that the real part of eigenvalues is limited by $R < 1$. The limitation could be achieved by dividing the complex eigenvalues by maximal absolute magnitude.
		\begin{equation} \label{eq:asym_normalization}
			\tilde{\lambda}_i = \frac{R \lambda_i}{\Vert\lambda\Vert_{\text{max}}} \, \, \, \, \, \forall i \, ,
		\end{equation}  
	The contribution of eigenvalues is influenced by the parameter $a$, which determines the proportion of symmetric interaction. In the case of only having symmetric interaction network, all eigenvalues are real and therefore the distribution forms a line in the complex plane. On the other hand, if $J$ is only a general asymmetric interaction network, the distribution forms a circle \cite{rajan2006eigenvalue}. For $a$ between $0$ and $1$, the distribution is a symmetric ellipse. 
		\begin{figure} [H]
			\centering
			\includegraphics[width=\textwidth]{../figures/asym_eigval_distribution.pdf}
			\caption{\textbf{Eigenvalue distribution in dependence of parameter $a$ in (\ref{eq:asym_interaction_matrix}).} In general, an asymmetric matrix has eigenvalues in complex plane. The degree of symmetry determines the form of distribution from a line ($a = 1$, only real eigenvalues) to a full circle ($a = 0$) with radius $R < 1$. Between $a = 0$ and $a = 1$, the distribution is a symmetric ellipse along imaginary part $=0$. Sub figures from left to right show the eigenvalue distribution in the complex plane from $a = 1, 0.5,$ and $1$. The gray line marks the radius $-R$ and $R$. Here $R = 0.85 < 1$.}
		\end{figure}
	
	\subsubsection{Modifications of Feedforward Recurrent Alignment for Asymmetric Interactions} \label{sec:modify_ffrec_alignment_score}
	% Reason for modification.
	% The three Hypothesis/Modification that were looked at. 
	We want to have the similar quantification of feedforward recurrent alignment as for the symmetric interaction matrix (section \ref{sec:ffrec_definition}). However, when aligning the feedforward input to eigenvectors of asymmetric recurrent interaction network $J$ like (\ref{eq:ffrec_equals_eigval}), the result calculated by (\ref{eq:ffrec_align}) is a complex number and can therefore be hardly interpreted. To embed such a sore with analogical idea, we deliberate the following modifications and try to discover the outcomes. 
	
	Aligning feedforward input $h \in \mathbb{C}^{n \times 1}$ to the asymmetric interaction $J$, we consider:
		\begin{enumerate}
			\item \label{sec:modicication_real_part} Only the real part of input $h$ and calculate the feedforward recurrent alignment with $\tilde{h} = \text{Re}(h) \in \mathbb{R}^{n \times 1}$.
				\begin{equation} \label{eq:ffrec_real_part}
					\nu_{\text{Re}} := \frac{\tilde{h}^T J \tilde{h}}{\Vert \tilde{h} \Vert^2} = \frac{\text{Re}(h)^T J \text{Re}(h)}{\Vert \text{Re}(h)\Vert^2} \, .
				\end{equation}
			\item \label{sec:modification_magnitude} Consider the magnitude of all entries in feedforward input $h$ and take $\tilde{h_i} = \vert h_i \vert \in \mathbb{R} \, \, \forall i$. Therefore $\tilde{h} \in \mathbb{R}^{n \times 1}$.
				\begin{equation} \label{eq:ffrec_mag}
					\nu_{\text{mag}} := \frac{\tilde{h}^T J \tilde{h}}{\Vert \tilde{h} \Vert^2} \text{ with } \tilde{h}_i = \vert h_i \vert \in \mathbb{R} \, .
				\end{equation}
			\item \label{sec:modification_symmetrized} Symmetrize $J$ through
				\begin{equation} \label{eq:symmetrized_J}
					\tilde{J} = \frac{J + J^T}{2} \, .
				\end{equation}
				Instead of align the feedforward input directly to $J$, we align indirectly to $\tilde{J}$ with its eigenvectors $\tilde{e}_i \in \mathbb{R}^{n \times 1}$. If having feedforward input aligned to eigenvectors to $J$ as in (\ref{eq:ffrec_equals_eigval}), the feedforward recurrent alignment will be calculated with eigenvectors of $\tilde{J}$ instead:
				\begin{equation} \label{eq:ffrec_symmetrized}
					\nu_{\text{sym}} =  \frac{\tilde{e}_i^T J \tilde{e}_i}{\Vert \tilde{e}_i \Vert^2} \in \mathbb{R} \, ,
				\end{equation}
				with $\tilde{\lambda}_i$ eigenvalues of $\tilde{J}$ . 
		\end{enumerate}
	
	\subsubsection{Related Modification for Evaluation} \label{sec:modification_asym}
	% Change in dimensionality 
	As for symmetric interaction network, four activity properties are taken into account to evaluate the feedforward recurrent alignment hypothesis with measurement of the alignment scores. In the case of asymmetric interaction, the modified alignment scores (section \ref{sec:modify_ffrec_alignment_score}). For repetition, the four properties are
		\begin{enumerate}
			\item Trial-to-trial correlation with (\ref{eq:ttc_sym}).
			\item Intra-trial stability with (\ref{eq:its_sym}).
			\item Dimensionality.% Modification for the covariance matrix and analytical dimensionality. 
			
				  With symmetric interaction networks, the covariance matrix for generation of inputs and responses is constructed with eigenvectors of interaction matrix since they build up a set of basis for $\mathbb{R}^{n \times n}$. But with asymmetric interaction matrix, the eigenvectors are basis for $\mathbb{C}^{n \times n}$. If using complex eigenvectors, the inputs and responses will be complex without plausible interpretations. Therefore, the construction of the covariance matrix need to be modified synchronously to have at least vectors from $\mathbb{R}^{n \times 1}$. 
				  
				  The same problem exists also for the analytical calculation for effective dimensionality (\ref{eq:effective_dimensionality_analytical}): we now have complex eigenvalues that lead to the dimensionality also complex. To overcome this problem, we work along the same modifications as above for covariance matrix.
				  
				  As a result, having $e_i$ eigenvectors and $\lambda_i$ of asymmetric interaction network $J$
				  \begin{itemize}
				  	\item For modification \ref{sec:modicication_real_part} (\ref{eq:ffrec_real_part}), applying $\tilde{e}_i = \text{Re}(e_i)$ for construction covariance matrix and $\tilde{\lambda}_i = \text{Re}\lambda_i$ for calculation of effective dimensionality. 
				  	\item For modification \ref{sec:modification_magnitude} (\ref{eq:ffrec_mag}), applying magnitude for all entries in $e_i$ to formulate $\tilde{e}_i$ and $\tilde{\lambda}_i = \vert \lambda_i \vert$ also magnitude of eigenvalues. 
				  	\item For modification \ref{sec:modification_symmetrized} (\ref{eq:ffrec_symmetrized}), applying eigenvectors $\tilde{e}_i$ and eigenvalues $\tilde{\lambda}_i$ from $\tilde{J}$ (\ref{eq:symmetrized_J}).
				  \end{itemize}
			  		
			  	  The covariance matrix for generating inputs is constructed similar to it with symmetric interaction network (\ref{eq:Sigma_dim}) but with $\tilde{e}_i$ defined above, 
			  	  	\begin{equation} \label{eq:modifications_dim}
			  	  		\Sigma^{\text{Dim}} := \sum_{i=L}^{L+M} \text{exp}\left(\frac{2(i-L)}{\beta}\right) \tilde{e}_i \tilde{e}_i^T \, .
			  	  	\end{equation}
		  	  	
		  	  	  Analogously, calculating the effective dimensionality defined by (\ref{eq:dim_analytical_sym}) but with $\tilde{\lambda}_i$,
		  	  	  	\begin{equation} \label{eq:modification_eff_dim}
		  	  	  		d^r_{\text{eff}} = \frac{\left(\sum_{i = L}^{L + M} \text{exp}\left(-2 \frac{i-L}{\beta}\right)(1-\tilde{\lambda}_i)^{-2}\right)^2}{\sum_{i=L}^{L+M} \text{exp}\left(-4 \frac{i-L}{\beta}\right)(1-\tilde{\lambda}_i)^{-4}}
		  	  	  	\end{equation}
		  	  	
			\item Alignment to spontaneous activity. % Modification due to dimensionality modification.
				  
				  Since the same formulation of covariance matrix with a larger parameter $\beta_{\text{spont}}$ is used for generation of broader endogenous inputs, the same modifications above for dimensionality (\ref{eq:modifications_dim}) can be taken over into covariance matrix of endogenous inputs like (\ref{eq:Sigma_spont}),
				  	\begin{equation} \label{eq:Sigma_spont_asym}
				  		\mathbf{\Sigma}^{\text{spont}} := \sum_{i=1}^{\kappa\beta_{\text{spont}}} \text{exp}\left(\frac{2(i-1)}{\beta_{\text{spont}}}\right) \tilde{e}_i \tilde{e}_i^T \, .
				  	\end{equation}
				  
		\end{enumerate}
	
	\clearpage
	\subsection{Low Rank Recurrent Network Model}
	% Reasons and importance for looking at low rank networks. (limitations of full rank RNN and advantages of low-rank RNN.)
	Until now, we considered full ranked random recurrent networks in both symmetric and asymmetric cases. Those fully recurrent connectivity structure is one of the most popular and best-studied classes of network models. However, randomly connected network display only very stereotyped responses to external inputs, can implement only a limited range of iniput-output computations \cite{mastrogiuseppe2018linking}. 
	
	Furthermore, experimental large-scale neural recordings have established that the transformation of sensory stimuli into motor outputs relies on low-dimensional dynamics at the population level \cite{mastrogiuseppe2018linking}. Besides, actual cortical connectivity appears to be neither fully random nor fully structured \cite{harris2013cortical}. Understanding how low-dimensional computations on mixed, distributed representations emerge from the structure of the recurrent connectivity and inputs to cortical networks is a major challenge \cite{mastrogiuseppe2018linking}. The neural computations can also be understood at the level of dynamical systems that govern low-dimensional trajectories of collective neural activity \cite{beiran2021shaping}. 
	
	Low-rank recurrent neural networks (RNNs) rely on connectivity matrices that are restricted to be low rank, which directly generate low-dimensional dynamics. The rank of the network determines the number of collective variables needed to provide a full description of the collective dynamics \cite{beiran2021shaping}. 
	
	\subsubsection{Construction of Low Rank Interactions} \label{sec:low_rank_construct}
	% Two different ways for constructing the low rank interactions. 
	% Construction of the symmetric and asymmetric low rank interactions. 
	Low-rank networks has the rank smaller than the number neurons. We found two possible constructions of low rank RNNs, which differs if the network is disturbed with Gaussian distributed random noise. 
	\paragraph{Low-rank RNNs without random noise \cite{beiran2021shaping, dubreuil2022role}.}
	Here, neurons in low-rank RNNs are organized in distinct populations that correspond to clusters in the space of low-rank connectivity patterns. Each population is defined by its statistics of connectivity, described by a multi-variate Gaussian distribution, so that the full network is specified by a mixture of Gaussians \cite{beiran2021shaping}.
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.6\textwidth]{../figures/low_rank_RNN_without_noise.jpg}
			\caption{\textbf{Low-rank recurrent networks (RNNs) constructed with distinct Gaussian distribution \cite{beiran2023parametric}}. A low-rank matrix could be written as a sum of outer products of vectors that are Gaussian distributed. As a result, the connectivity matrix is a mixture of Gaussians. }
			\label{fig:low_rank_RNN_without_noise}
		\end{figure}
	The connection matrix is constructed as a $n \times n$ dimensional matrix $J$ where $n$ is the number of neurons. Using singular value decomposition, the connectivity matrix with rank $G \ll n$ can be expressed as the sum of $G$ unit rank terms
		\begin{equation} \label{eq:low_rank_RNN_without_noise}
			J_{ij} = \frac{1}{n}\sum_{g = 1}^{G} l_i^{(g)}r_j^{(g)} \, \, \text{or} \, \, J = \frac{1}{n} \sum_{g=1}^{G} l^{(g)} r^{(g)T} \, . 
		\end{equation}
	The connectivity is therefore characterized by a set of $G$ $n$-dimensional vectors, or the $g$-th connectivity patterns, $l^{(g)} = \left\{l_i^{(g)}\right\}_{i = 1, ..., n } \in \mathbb{R}^{n \times 1}$ and $r^{(g)} = \left\{r_i^{(g)}\right\}_{i = 1, ..., n } \in \mathbb{R}^{n \times 1}$ for $g = 1, ..., G$. $\left\{l^{(g)}\right\}$ are the left singular vectors of the connectivity matrix and $\left\{r^{(g)}\right\}$ the right. The vectors $\left\{l^{(g)}\right\}$ and $\left\{r^{(g)}\right\}$ are mutually orthogonal and randomly multi-variate Gaussian distributed \cite{beiran2021shaping}. 
	
	\paragraph{Low-rank RNNs with random noise \cite{mastrogiuseppe2018linking}.}
	Similar to the construction of low-rank connection above (figure \ref{fig:low_rank_RNN_without_noise} defined by (\ref{eq:low_rank_RNN_without_noise})), \cite{mastrogiuseppe2018linking} suggested that the connectivity matrix can be constructed with a part $P$ to be fixed and known and a random uncorrelated part.
		\begin{figure} [H]
			\centering
			\includegraphics[width=0.75\textwidth]{../figures/low_rank_RNN_with_noise.pdf}
			\caption{\textbf{Low-rank recurrent networks (RNNs) constructed with fixed part and random noise \cite{mastrogiuseppe2018linking}.} The connectivity matrix is given by a sum of an uncontrolled, random matrix and a structured, controlled matrix $P$. Except for the fixed and know part $P$, which is a mixture of uncorrelated Gaussians, the RNN is disturbed by random noise.}
			\label{fig:low_rank_RNN_with_noise}
		\end{figure}
	The fixed part $P$ has the same construction as the network without noise (\ref{eq:low_rank_RNN_without_noise}). The uncontrolled random matrix can be constructed in a complex way with certain current-to-rate transfer function \cite{mastrogiuseppe2018linking}. 
	%For simplicity, we will try the random noise with a fully Gaussian distributed matrix. 
	That is
		\begin{equation} \label{eq:low_rank_with_noise}
			J = \frac{1}{n} \sum_{g=1}^{G} l^{(g)} r^{(g)T} + J_{\text{rand}} \, ,
		\end{equation}
	with $J_{\text{rand}}$ a $n \times n$ random matrix. 
	
	\subsubsection{Feedforward Recurrent Alignment Hypothesis with Low-rank RNNs} \label{sec:ffrec_low_rank}
	To test the feedforward recurrent alignmnent hypothesis applying the low-rank RNNs, we firstly consider the simple case of having symmetric low-rank connectivity, because symmetric structure is easier to interpret. Then we tried the asymmetric low-rank RNNs to discover the influence of rank on response properties. 
	
	\paragraph{Symmetric low-rank RNN}This could be achieved through choosing the left connectivity vectors $\left\{l^{(g)}\right\}$ equal the right connectivity vectors $\left\{r^{(g)}\right\}$. As a result the low-rank RNN without noise can be formulated by sum of symmetric matrices and therefore also symmetric:
		\begin{equation} \label{eq:sym_low_rank}
			J = \frac{1}{n}\sum_{g =1}^{G} l^{(g)} l^{(g)T} \, \, \text{or} \, \, J = \frac{1}{n}\sum_{g =1}^{G} r^{(g)} r^{(g)T} \, .
		\end{equation}
	If considering the connectivity matrix with noise, for simplicity, we add a random $n \times n$ symmetrized Gaussian distributed matrix to $J$ from (\ref{eq:sym_low_rank}).
	
	The dynamics and conditions for stable stability of the response in the symmetric low-rank RNNs should be kept the same as with symmetric full rank RNNs (\ref{eq:sym_response_ODE}, \ref{eq:sym_stable_fix_point}). In order to keep the steady state of responses stable, the eigenvalues $\lambda_i \in \mathbb{R}$ of the low-rank RNNs $J$ (\ref{eq:sym_low_rank}) are limited by $R < 1$ through normalizing with the maximal eigenvalue (\ref{eq:eigval_normal}). 
	
	\paragraph{Asymmetric low-rank RNN} When the set of left connectivity vector  $\left\{l^{(g)}\right\}$ is different from the set of right connectivity vector $\left\{r^{(g)}\right\}$, we receive the asymmetric low-rank with (\ref{eq:low_rank_RNN_without_noise}) for the case without random noise. If considering the random noise, we again add a simple $n \times n$ Gaussian distributed full rank asymmetric matrix as figure \ref{fig:low_rank_RNN_with_noise} illustrated (\ref{eq:low_rank_with_noise}). 
	
	To enable the stable steady state of responses, as shown in asymmetric RNNs (\ref{eq:asym_stable_fix_point}), the real part of eigenvalues $\text{Re}(\lambda_i)$ of asymmetric RNNs $J$ (\ref{eq:low_rank_with_noise}) are limited by $R < 1$ through normalization with the maximal magnitude of eigenvalues (\ref{eq:asym_normalization}). 
	
	\subsubsection{Evaluation of Feedforward Recurrent Alignment Hypothesis Based on Response Properties}
	Similar to the analysis on full rank networks, we also want to evaluate the modulation of feedforward recurrent alignment for low-rank RNNs based on the four response properties in correlation with feedforward recurrent alignment: 
		\begin{itemize}
			\item Trial-to-trial correlation
			\item Intra-trial stability
			\item Dimensionality 
			\item Alignment of evoked activity pattern to spontaneous activity pattern
		\end{itemize}
	
	\paragraph{Symmetric low-rank RNN} For symmetric low-rank RNN, the methods for modulations and evaluations are traced back to the full rank symmetric RNN before. The feedforward recurrent alignment is defined by (\ref{eq:ffrec_align}). Moreover, the trial-to-trial correlation is quantified with (\ref{eq:ttc_sym}), the intra-trial stability with (\ref{eq:its_sym}), the dimensionality with (\ref{eq:dim_analytical_sym}) and (\ref{eq:dim_empirical_sym}), and at the end the alignment to spontaneous activity with (\ref{eq:align_to_spont_act_sym}). 
	
	\paragraph{Asymmetric low-rank RNN} Since for asymmetric low-rank RNNs, the problem of complex eigenvectors and eigenvalues still exists, the modifications undertaken at asymmetric full rank RNNs (section \ref{sec:modify_ffrec_alignment_score} and \ref{sec:modification_asym}) can therefore be directly applied here, when align the inputs to the RNN. Generally, the modifications that were considered can be roughly described as: 1) only consider real part of eigenvalues and eigenvectors, 2) consider for each neuron the magnitude of inputs and variance ratio, 3) align the inputs to symmetrized network to approximate. 
	
	\clearpage
	\subsection{Black Box Recurrent Network Model}
	% Background
	Until now, we assume that we already know the structure of recurrent networks and evaluate the feedforward recurrent alignment hypothesis on the networks. However, in reality during the experiments, mostly only a small part of the network is known and the total structure of the network keeps mostly like a black box. As a result, the eigenvectors of the networks are generally unknown. The feedforward recurrent alignment cannot use the dominant modes to characterize the development of feedforward recurrent network system. 
	
	% Motivation from multiple experiments. Literatures
	It was pointed out that the reliability of evoked dynamics in recurrent networks is dependent on the stimulus used. As a consequence, to a recurrent network would correspond a set of stimuli which are more efficiently transmitted than others \cite{marre2009reliable}. 
	Especially the stimulus inputs that align with the structure of endogenous sub-networks would be recurrently amplified, leading to more reliable evoked responses \cite{mulholland2023selective}. 
	
	Besides, the similarity between spontaneous and evoked activity in sensory cortical areas could be a signature of efficient transmission and propagation across cortical networks. Based on a better recall caused by a match between spontaneous activity and input statistics, it was hypothesized that the recurrent connectivity could have been shaped by a learning process so that the spontaneous activity matches the natural input statistics \cite{marre2009reliable}. 
	
	% Explanation of the intuition. Want to approximate the dominant modes.
	Therefore, we wonder if we could apply the experimental measurable spontaneous activity to characterize the feedforward recurrent alignment. Without knowing the eigenvectors of the recurrent networks, align feedforward inputs to the spontaneous activity instead of the recurrent networks. This can be a further modification for modeling feedforward recurrent alignment for general asymmetric recurrent networks. 
	
	Furthermore, we consider to repeatedly apply the recurrently amplified spontaneous stimuli as inputs to discover if the amplified spontaneous activity could increase the alignment to the recurrent network. 
	
	\subsubsection{Approximation with White Noise Evoked Activity} % Low rank is here not necessary (could perhaps shortly in Appendix).
	
	With unknown recurrent structure, which we assume to be asymmetric generally, it is then difficult to find the stimuli pattern such that the trial-to-trial correlation, intra-trial stability and alignment between evoked activity to spontaneous activity to be high while keeping dimensionality low. In other words, we cannot apply the eigenvalues of the recurrent network to align with inputs and then characterize the development of feedforward inputs leading to stable response properties. 
	
	Alternatively, we align the inputs to spontaneous pattern and explore the response properties correlation with feedforward recurrent alignment. 
	
	% Definition of feedforward recurrent alignment with principal component.
	Spontaneous activity is evoked by white noise $h \in \mathbb{R}^{n \times 1}$, which is modulated by multivariate normal distribution with mean zero vector $0_v \in \mathbb{R}^{n \times 1}$ and covariance matrix the identity matrix $I_n \in \mathbb{R}^{n \times n}$, 
		\begin{equation}
			h \sim \mathcal{N}(0_v, I_n) \, .
		\end{equation}
	The spontaneous activity is then modeled by the transformed steady state response $r \in \mathbb{R}^{n \times 1}$, which is also multivariate normal distributed with transformed covariance matrix, 
		\begin{equation}
			r \sim \mathcal{N}\left(0_v, (1-J)^{-1}(1-J)^{-T}\right) \, .
		\end{equation}
	The response pattern is determined by the covariance matrix. If align the inputs to response patterns, the eigenvectors of covariance matrices are aligned. The eigenvectors of a covariance matrix are also known as principal components for the distribution. 
	
	To model the feedforward recurrent alignment hypothesis, the inputs are aligned to principal components and the feedforward alignment score is formulated with principal components instead of with modified eigenvectors of asymmetric recurrent network like section \ref{sec:modify_ffrec_alignment_score}. For a input $h$ aligned to a principal component $p$, the feedforward recurrent alignment is constructed with 
		\begin{equation}
			\nu := \frac{p^T J p}{\Vert p \Vert^2} \, .
		\end{equation}
	
	% Evaluation of the modification. monotony, ttc, its, dim, align to spont.act
	The original recurrent network is now approximated by the spontaneous response pattern and the inputs are aligned to principal components of the spontaneous response pattern. As for the recurrent networks, some properties of the new formulated feedforward recurrent alignment score have to be evaluated. The perspectives that we take into account are:
		\begin{itemize}
			\item Monotonously growing correlation between feedforward recurrent alignment score and eigenvalues of covariance matrix from spontaneous activity pattern.
			\item Positive correlation between feedforward recurrent alignment score and trial-to-trial correlation.
			\item Positive correlation between feedforward recurrent alignment score and intra-trial stability.
			\item Negative correlation between feedfroward recurrent alignment score and dimensionality.
			\item Feedforward recurrent alignment score is positive correlated with alignment of evoked activity to spontaneous activity.
		\end{itemize}
	
	% Monotony Method.
	\paragraph{Monotony}
	
	% ttc method.
	
	% its method.
	
	% dim method
	
	% alignment to spont. act
	
	\subsubsection{Iterative Approximation with Low Dimensional Inputs}
	
	\clearpage
	\subsection{Hebbian Learning in Feedforward Recurrent Networks}
	% Following the iterative approximation from the last section. Motivation. The goal.
	\subsubsection{Model Setting}
	% One input neuron model. Figure for the model construction. 
	\subsubsection{Update Rules for Feedforward Network}
	% The Differential equations for the update rules.
	% Development and the derivative of the feedforward recurrent alignment.
	\subsubsection{Projection of the Feedforward Weights on Eigenvectors}
	% What do the projects and how the projects are calculated.
	
	
\end{document}