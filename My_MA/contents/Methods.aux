\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Methods}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Symmetric Recurrent Network Model}{1}{subsection.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces \textbf  {Illustration of symmetric recurrent networks (symmetric RNNs).}   \textbf  {(a)} An example of symmetric connections between two neurons in a network with multiple neurons. If there are connections between two neurons, here for example the green and red neurons, the directed connection from the green neuron to the red neuron has the same strength as the directed connection from the red to the green. \textbf  {(b)} Structure of a symmetric RNN with feedforward inputs as the inputs for the recurrent layer. The connections between neurons inside the recurrent layer are symmetric, as illustrated in figure (a).\relax }}{1}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:symmetric_RNN}{{1.1}{1}{\textbf {Illustration of symmetric recurrent networks (symmetric RNNs).} \\ \textbf {(a)} An example of symmetric connections between two neurons in a network with multiple neurons. If there are connections between two neurons, here for example the green and red neurons, the directed connection from the green neuron to the red neuron has the same strength as the directed connection from the red to the green. \textbf {(b)} Structure of a symmetric RNN with feedforward inputs as the inputs for the recurrent layer. The connections between neurons inside the recurrent layer are symmetric, as illustrated in figure (a).\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Symmetric Recurrent Interaction}{1}{subsubsection.4}\protected@file@percent }
\newlabel{sec:symmetric_recurrent_interaction}{{1.1.1}{1}{Symmetric Recurrent Interaction}{subsubsection.4}{}}
\newlabel{eq:gaussian_distribution}{{1.1}{1}{Symmetric Recurrent Interaction}{equation.5}{}}
\citation{dayan2005theoretical}
\newlabel{eq:eigval_normal}{{1.3}{2}{Symmetric Recurrent Interaction}{equation.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Response Steady State}{2}{subsubsection.8}\protected@file@percent }
\newlabel{sec:steady_state_response_sym}{{1.1.2}{2}{Response Steady State}{subsubsection.8}{}}
\@writefile{toc}{\contentsline {paragraph}{Existence of Steady State}{2}{section*.9}\protected@file@percent }
\newlabel{eq:sym_response_ODE}{{1.4}{2}{Existence of Steady State}{equation.10}{}}
\newlabel{eq:steady_state}{{1.5}{2}{Existence of Steady State}{equation.11}{}}
\@writefile{toc}{\contentsline {paragraph}{Stability of Steady State}{2}{section*.12}\protected@file@percent }
\citation{tragenap2023nature}
\newlabel{eq:Jacobian_matrix}{{1.8}{3}{Stability of Steady State}{equation.15}{}}
\newlabel{eq:steady_state_eigenvalues_sym}{{1.9}{3}{Stability of Steady State}{equation.17}{}}
\newlabel{eq:sym_stable_fix_point}{{1.10}{3}{Stability of Steady State}{equation.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.3}Feedforward Recurrent Alignment for Symmetric Interactions}{3}{subsubsection.19}\protected@file@percent }
\newlabel{sec:ffrec_definition}{{1.1.3}{3}{Feedforward Recurrent Alignment for Symmetric Interactions}{subsubsection.19}{}}
\newlabel{eq:ffrec_align}{{1.11}{3}{Feedforward Recurrent Alignment for Symmetric Interactions}{equation.20}{}}
\newlabel{eq:h_prop_eigvec}{{1.12}{3}{Feedforward Recurrent Alignment for Symmetric Interactions}{equation.21}{}}
\citation{tragenap2023nature}
\citation{Soch2019}
\citation{tragenap2023nature}
\citation{tragenap2023nature}
\newlabel{eq:ffrec_equals_eigval}{{1.13}{4}{Feedforward Recurrent Alignment for Symmetric Interactions}{equation.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.4}Response Properties for Evaluation}{4}{subsubsection.23}\protected@file@percent }
\newlabel{sec:response_properties_for_evaluation}{{1.1.4}{4}{Response Properties for Evaluation}{subsubsection.23}{}}
\newlabel{para:ttc_sym}{{1.1.4}{4}{Trial-to-trial correlation}{section*.24}{}}
\@writefile{toc}{\contentsline {paragraph}{Trial-to-trial correlation}{4}{section*.24}\protected@file@percent }
\newlabel{eq:input_distribution}{{1.14}{4}{Trial-to-trial correlation}{equation.25}{}}
\newlabel{eq:steady_state_distribute}{{1.15}{4}{Trial-to-trial correlation}{equation.26}{}}
\newlabel{eq:ttc_sym}{{1.18}{4}{Trial-to-trial correlation}{equation.29}{}}
\citation{tragenap2023nature}
\@writefile{toc}{\contentsline {paragraph}{Intra-trial stability}{5}{section*.30}\protected@file@percent }
\newlabel{eq:sde_intra_trial_stability}{{1.19}{5}{Intra-trial stability}{equation.31}{}}
\newlabel{eq:its_sym}{{1.24}{5}{Intra-trial stability}{equation.38}{}}
\citation{tragenap2023nature}
\citation{tragenap2023nature}
\@writefile{toc}{\contentsline {paragraph}{Dimensionality}{6}{section*.39}\protected@file@percent }
\newlabel{eq:input_distribution_dimensionality}{{1.25}{6}{Dimensionality}{equation.40}{}}
\newlabel{eq:response_distribution_dimensionality}{{1.26}{6}{Dimensionality}{equation.41}{}}
\newlabel{eq:Sigma_dim}{{1.27}{6}{Dimensionality}{equation.42}{}}
\newlabel{eq:effective_dimensionality_analytical}{{1.28}{6}{Dimensionality}{equation.43}{}}
\newlabel{eq:response_variance_ratio_dimensionality}{{1.30}{6}{Dimensionality}{equation.45}{}}
\citation{tragenap2023nature}
\newlabel{eq:dim_analytical_sym}{{1.31}{7}{Dimensionality}{equation.46}{}}
\newlabel{eq:dim_empirical_sym}{{1.32}{7}{Dimensionality}{equation.47}{}}
\@writefile{toc}{\contentsline {paragraph}{Alignment with spontaneous activity}{7}{section*.48}\protected@file@percent }
\newlabel{eq:var_explain_spont_act_sym}{{1.33}{7}{Alignment with spontaneous activity}{equation.49}{}}
\newlabel{eq:align_to_spont_act_sym}{{1.34}{7}{Alignment with spontaneous activity}{equation.50}{}}
\newlabel{eq:Sigma_spont}{{1.37}{8}{Alignment with spontaneous activity}{equation.53}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Asymmetric Recurrent Network Model}{9}{subsection.54}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces \textbf  {Illustration of asymmetric recurrent networks (asymmetric RNNs).} In general, asymmetric recurrent networks do not have symmetric interaction strength between to neurons. \textbf  {(a)} An example of asymmetric connections between two neurons in a network with multiple neurons. There are connections between the green and red neurons. The connection from green one to red one is stronger than the connection from the red to green. There are also connections that only from one neuron to the other but no connection back from the other neuron. \textbf  {(b)} Structure of a asymmetric RNN with feedforward inputs as the inputs for the recurrent layer. The connections between neurons inside the recurrent layer are asymmetric, as illustrated in figure (a).\relax }}{9}{figure.caption.55}\protected@file@percent }
\newlabel{fig:asymmetric_RNN}{{1.2}{9}{\textbf {Illustration of asymmetric recurrent networks (asymmetric RNNs).} In general, asymmetric recurrent networks do not have symmetric interaction strength between to neurons. \textbf {(a)} An example of asymmetric connections between two neurons in a network with multiple neurons. There are connections between the green and red neurons. The connection from green one to red one is stronger than the connection from the red to green. There are also connections that only from one neuron to the other but no connection back from the other neuron. \textbf {(b)} Structure of a asymmetric RNN with feedforward inputs as the inputs for the recurrent layer. The connections between neurons inside the recurrent layer are asymmetric, as illustrated in figure (a).\relax }{figure.caption.55}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Asymmetric Recurrent Interaction}{9}{subsubsection.56}\protected@file@percent }
\newlabel{eq:asym_interaction_matrix}{{1.38}{9}{Asymmetric Recurrent Interaction}{equation.57}{}}
\citation{rajan2006eigenvalue}
\newlabel{eq:asym_stable_fix_point}{{1.41}{10}{Asymmetric Recurrent Interaction}{equation.60}{}}
\newlabel{eq:asym_normalization}{{1.42}{10}{Asymmetric Recurrent Interaction}{equation.61}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces \textbf  {Eigenvalue distribution in dependence of parameter $a$ in (\ref  {eq:asym_interaction_matrix}).} In general, an asymmetric matrix has eigenvalues in complex plane. The degree of symmetry determines the form of distribution from a line ($a = 1$, only real eigenvalues) to a full circle ($a = 0$) with radius $R < 1$. Between $a = 0$ and $a = 1$, the distribution is a symmetric ellipse along imaginary part $=0$. Sub figures from left to right show the eigenvalue distribution in the complex plane from $a = 1, 0.5,$ and $1$. The gray line marks the radius $-R$ and $R$. Here $R = 0.85 < 1$.\relax }}{11}{figure.caption.62}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}Modifications of Feedforward Recurrent Alignment for Asymmetric Interactions}{11}{subsubsection.63}\protected@file@percent }
\newlabel{sec:modify_ffrec_alignment_score}{{1.2.2}{11}{Modifications of Feedforward Recurrent Alignment for Asymmetric Interactions}{subsubsection.63}{}}
\newlabel{sec:modicication_real_part}{{1}{11}{Modifications of Feedforward Recurrent Alignment for Asymmetric Interactions}{Item.64}{}}
\newlabel{eq:ffrec_real_part}{{1.43}{11}{Modifications of Feedforward Recurrent Alignment for Asymmetric Interactions}{equation.65}{}}
\newlabel{sec:modification_magnitude}{{2}{11}{Modifications of Feedforward Recurrent Alignment for Asymmetric Interactions}{Item.66}{}}
\newlabel{eq:ffrec_mag}{{1.44}{11}{Modifications of Feedforward Recurrent Alignment for Asymmetric Interactions}{equation.67}{}}
\newlabel{sec:modification_symmetrized}{{3}{11}{Modifications of Feedforward Recurrent Alignment for Asymmetric Interactions}{Item.68}{}}
\newlabel{eq:symmetrized_J}{{1.45}{11}{Modifications of Feedforward Recurrent Alignment for Asymmetric Interactions}{equation.69}{}}
\newlabel{eq:ffrec_symmetrized}{{1.46}{12}{Modifications of Feedforward Recurrent Alignment for Asymmetric Interactions}{equation.70}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.3}Related Modification for Evaluation}{12}{subsubsection.71}\protected@file@percent }
\newlabel{sec:modification_asym}{{1.2.3}{12}{Related Modification for Evaluation}{subsubsection.71}{}}
\newlabel{eq:modifications_dim}{{1.47}{13}{Related Modification for Evaluation}{equation.75}{}}
\newlabel{eq:modification_eff_dim}{{1.48}{13}{Related Modification for Evaluation}{equation.76}{}}
\newlabel{eq:Sigma_spont_asym}{{1.49}{13}{Related Modification for Evaluation}{equation.78}{}}
\citation{mastrogiuseppe2018linking}
\citation{mastrogiuseppe2018linking}
\citation{harris2013cortical}
\citation{mastrogiuseppe2018linking}
\citation{beiran2021shaping}
\citation{beiran2021shaping}
\citation{beiran2021shaping}
\citation{dubreuil2022role}
\citation{beiran2021shaping}
\citation{beiran2023parametric}
\citation{beiran2023parametric}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Low Rank Recurrent Network Model}{14}{subsection.79}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Construction of Low Rank Interactions}{14}{subsubsection.80}\protected@file@percent }
\newlabel{sec:low_rank_construct}{{1.3.1}{14}{Construction of Low Rank Interactions}{subsubsection.80}{}}
\@writefile{toc}{\contentsline {paragraph}{Low-rank RNNs without random noise \cite  {beiran2021shaping, dubreuil2022role}.}{14}{section*.81}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces \textbf  {Low-rank recurrent networks (RNNs) constructed with distinct Gaussian distribution \cite  {beiran2023parametric}}. A low-rank matrix could be written as a sum of outer products of vectors that are Gaussian distributed. As a result, the connectivity matrix is a mixture of Gaussians. \relax }}{14}{figure.caption.82}\protected@file@percent }
\newlabel{fig:low_rank_RNN_without_noise}{{1.4}{14}{\textbf {Low-rank recurrent networks (RNNs) constructed with distinct Gaussian distribution \cite {beiran2023parametric}}. A low-rank matrix could be written as a sum of outer products of vectors that are Gaussian distributed. As a result, the connectivity matrix is a mixture of Gaussians. \relax }{figure.caption.82}{}}
\citation{beiran2021shaping}
\citation{mastrogiuseppe2018linking}
\citation{mastrogiuseppe2018linking}
\citation{mastrogiuseppe2018linking}
\citation{mastrogiuseppe2018linking}
\citation{mastrogiuseppe2018linking}
\newlabel{eq:low_rank_RNN_without_noise}{{1.50}{15}{Low-rank RNNs without random noise \cite {beiran2021shaping, dubreuil2022role}}{equation.83}{}}
\@writefile{toc}{\contentsline {paragraph}{Low-rank RNNs with random noise \cite  {mastrogiuseppe2018linking}.}{15}{section*.84}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces \textbf  {Low-rank recurrent networks (RNNs) constructed with fixed part and random noise \cite  {mastrogiuseppe2018linking}.} The connectivity matrix is given by a sum of an uncontrolled, random matrix and a structured, controlled matrix $P$. Except for the fixed and know part $P$, which is a mixture of uncorrelated Gaussians, the RNN is disturbed by random noise.\relax }}{15}{figure.caption.85}\protected@file@percent }
\newlabel{fig:low_rank_RNN_with_noise}{{1.5}{15}{\textbf {Low-rank recurrent networks (RNNs) constructed with fixed part and random noise \cite {mastrogiuseppe2018linking}.} The connectivity matrix is given by a sum of an uncontrolled, random matrix and a structured, controlled matrix $P$. Except for the fixed and know part $P$, which is a mixture of uncorrelated Gaussians, the RNN is disturbed by random noise.\relax }{figure.caption.85}{}}
\newlabel{eq:low_rank_with_noise}{{1.51}{15}{Low-rank RNNs with random noise \cite {mastrogiuseppe2018linking}}{equation.86}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}Feedforward Recurrent Alignment Hypothesis with Low-rank RNNs}{16}{subsubsection.87}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Symmetric low-rank RNN}{16}{section*.88}\protected@file@percent }
\newlabel{eq:sym_low_rank}{{1.52}{16}{Symmetric low-rank RNN}{equation.89}{}}
\@writefile{toc}{\contentsline {paragraph}{Asymmetric low-rank RNN}{16}{section*.90}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3}Evaluation of Feedforward Recurrent Alignment Hypothesis Based on Response Properties}{16}{subsubsection.91}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Symmetric low-rank RNN}{17}{section*.92}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Asymmetric low-rank RNN}{17}{section*.93}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Black Box Recurrent Network Model}{18}{subsection.94}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}Approximation with White Noise Evoked Activity}{18}{subsubsection.95}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.2}Iterative Approximation with Low Dimensional Inputs}{18}{subsubsection.96}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Hebbian Learning in Feedforward Recurrent Networks}{19}{subsection.97}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.1}Model Setting}{19}{subsubsection.98}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.2}Update Rules for Feedforward Network}{19}{subsubsection.99}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.5.3}Projection of the Feedforward Weights on Eigenvectors}{19}{subsubsection.100}\protected@file@percent }
\gdef \@abspage@last{19}
